{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# 텍스트 데이터를 이용한 텍스트 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:45:51.456867Z",
     "start_time": "2023-04-04T06:45:51.337582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "clean_data = pd.read_csv('twitter-airline-sentiment.csv')\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Pre-Processing\n",
    "\n",
    "텍스트 전처리 단계에는 사용 가능한 텍스트 데이터를 추가로 정리하기 위한 몇 가지 필수 작업이 포함됩니다. 여기에는 다음과 같은 작업이 포함됩니다.\n",
    "\n",
    "**1. 불용어 (stopword) 제거** : 영어에서 a, an, the, as, in, on 등과 같은 단어는 불용어로 간주되므로 요구 사항에 따라 이러한 단어를 제거하여 어휘 크기를 줄일 수 있습니다. 어떤 특정한 의미를 가지고\n",
    "\n",
    "**2. 소문자 변환 :** 대소문자가 문제에 영향을 미치지 않을 수 있으므로 모든 단어를 소문자로 변환합니다. 그렇게 함으로써 우리는 어휘의 크기를 줄이고 있습니다.\n",
    "\n",
    "**3. 형태소 분석 :** 형태소 분석은 접미사를 제거하고 해당 단어의 모든 다른 변형이 동일한 형태로 표현될 수 있도록 단어를 일부 기본 형태로 줄이는 과정을 말합니다(예: \"walk\"와 \"walking\"은 모두 \" 걷다\").\n",
    "\n",
    "**4. 토큰화 :** NLP 소프트웨어는 일반적으로 텍스트를 단어(토큰)와 문장으로 분해하여 텍스트를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:45:52.354324Z",
     "start_time": "2023-04-04T06:45:52.336088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요 없는 컬럼명 제거\n",
    "waste_col = ['tweet_id', 'airline_sentiment_confidence',\n",
    "       'negativereason', 'negativereason_confidence', 'airline',\n",
    "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
    "       'retweet_count', 'tweet_coord', 'tweet_created',\n",
    "       'tweet_location', 'user_timezone']\n",
    "\n",
    "data = clean_data.drop(waste_col, axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:45:53.146570Z",
     "start_time": "2023-04-04T06:45:52.921384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ebdl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk 라이브러리를 이용해 불용어, 소문자 변환, 형태소 분석, 토큰화 진행\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# preprocess_text라는 함수를 생성하여 문장이 들어오면 \n",
    "# 불용어 처리, 소문자 변환, 형태소 분석, 토큰화 진행를 하는 함수를 생성하세요.\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "\n",
    "    # 불용어(stopwords) 처리를 위해 영어 불용어 리스트를 불러옵니다.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 문장을 소문자로 변환합니다.\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 정규 표현식을 이용해 문장을 토큰화합니다.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # 형태소 분석을 위해 스노우볼 스태머 객체를 생성합니다.\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # 토큰 중에서 불용어가 아닌 단어에 대해 형태소 분석을 하고, 스태머를 적용합니다.\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 처리된 토큰 리스트를 반환합니다.\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:46:01.210349Z",
     "start_time": "2023-04-04T06:45:53.618641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Text : @VirginAmerica What @dhepburn said.\n",
      "\n",
      "Preprocessed Text : ['virginamerica', 'dhepburn', 'said']\n"
     ]
    }
   ],
   "source": [
    "# preprocess_text 생성하고 출력 테스트\n",
    "\n",
    "print(f\"Orignal Text : {data.text[0]}\")\n",
    "print()\n",
    "print(f\"Preprocessed Text : {preprocess_text(data.text[0])}\")\n",
    "\n",
    "# 기존 data에 preprocess_text를 적용하여 모든 문장을 전처리\n",
    "preprocessed_data = []\n",
    "\n",
    "for sentence in data.text:\n",
    "    preprocessed_sentence = preprocess_text(sentence)\n",
    "    preprocessed_data.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding\n",
    "\n",
    "원-핫 인코딩에서 말뭉치 어휘의 각 단어 w에는 1과 |V| 사이의 고유한 정수 ID(wid)가 지정되며, 여기서 V는 말뭉치 어휘 집합입니다. 그런 다음 각 단어는 0과 1의 V 차원 이진 벡터로 표시됩니다. 이것은 |V| 인덱스를 제외하고 모두 0으로 채워진 차원 벡터입니다. 여기서 index = wid입니다. 이 인덱스에서 우리는 단순히 1을 넣습니다. 그런 다음 개별 단어에 대한 표현이 결합되어 문장 표현을 형성합니다.\n",
    "\n",
    "예시: \n",
    "One Hot Representation for sentence \"the cat sat on the mat\" :\n",
    "\n",
    "![](https://miro.medium.com/max/886/1*_da_YknoUuryRheNS-SYWQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:46:01.348734Z",
     "start_time": "2023-04-04T06:46:01.320503Z"
    }
   },
   "outputs": [],
   "source": [
    "# 위의 예시를 이용한 vocab\n",
    "sample_vocab = ['the', 'cat', 'sat', 'on', 'mat', 'dog', 'run', 'green', 'tree']\n",
    "\n",
    "# 현재 전처리된 데이터셋을 가지고 data_vocab에 해당하는 vocab 셋 생성\n",
    "data_vocab = []\n",
    "\n",
    "# 데이터셋에 있는 모든 단어들을 하나의 리스트로 만듭니다.\n",
    "all_words = [word for sentence in preprocessed_data for word in sentence]\n",
    "\n",
    "data_vocab = set(all_words)\n",
    "data_vocab =list(data_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:54:36.594806Z",
     "start_time": "2023-04-04T06:54:36.583323Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot 인코딩을 실행하는 아래 함수 생성\n",
    "def get_onehot_representation(text, vocab = data_vocab):\n",
    "    onehot_encoded = []\n",
    "    for word in text:\n",
    "        onehot_encoded.append([1 if word == v else 0 for v in vocab])\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:54:37.050125Z",
     "start_time": "2023-04-04T06:54:37.035913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Representation for sentence \"the cat sat on the mat\" :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래 코드를 위의 예시 처럼 생성 되는지 출력 테스트\n",
    "print(\"One Hot Representation for sentence \\\"the cat sat on the mat\\\" :\")\n",
    "get_onehot_representation(['the', 'cat', 'sat', 'on', 'the', 'mat'], sample_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T06:57:57.790368Z",
     "start_time": "2023-04-04T06:57:57.781886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary : 12256\n",
      "Sample of Vocabulary : ['l0i8fnz3ku', 'clincher', 'investig', 'sugafli', 'decent', '85832', 'angryairtravel', 'gate', 'obscen', 'unfamiliar']\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드를 출력\n",
    "print(f'Length of Vocabulary : {len(data_vocab)}')\n",
    "print(f'Sample of Vocabulary : {data_vocab[302 : 312]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words (Binary Encoding, Count-based Encoding)\n",
    "\n",
    "BoW(Bag of Words)는 NLP, 특히 텍스트 분류 문제에서 일반적으로 사용된 고전적인 텍스트 표현 기술입니다. 핵심 아이디어는 다음과 같습니다. 순서와 문맥을 무시하면서 고려 중인 텍스트를 단어 모음으로 표현합니다.\n",
    "\n",
    "원-핫 인코딩과 유사하게 BoW는 단어를 1과 |V| 사이의 고유한 정수 ID로 매핑합니다. 말뭉치의 각 문서는 |V|의 벡터로 변환됩니다. 차원은 벡터의 i번째 구성요소에 있었습니다. i = wid는 문서에서 단어 w가 나타나는 횟수입니다. 즉, V의 각 단어를 문서에서 발생한 횟수로 점수를 매깁니다.\n",
    "\n",
    "아래의 예시를 들어 알아보자.\n",
    "\n",
    "단어로 구성된 어휘 V가 있다고 가정해 보겠습니다. **V --> {the, cat, sat, in, hat, with}** 그러면 몇 문장의 bag of word 표현은 다음과 같이 주어집니다.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*3IACMnNpwVlCl8kSTJocPA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T07:04:40.502223Z",
     "start_time": "2023-04-04T07:04:40.488735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary mapping for given sample corpus : \n",
      " {'the': 4, 'cat': 0, 'sat': 3, 'in': 2, 'hat': 1, 'with': 5}\n",
      "\n",
      "Bag of word Representation of sentence 'the cat cat sat in the hat'\n",
      "[[2 1 1 1 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn의 CountVectorizer를 통해서 하나의 샘플 문장을 Bag of Words 형태로 바꿀 수 있다.\n",
    "# 아래의 코드를 실행해보고 이해하여라.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_bow = CountVectorizer()\n",
    "\n",
    "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
    "\n",
    "sample_bow.fit(sample_corpus)\n",
    "\n",
    "def get_bow_representation(text):\n",
    "        return sample_bow.transform(text)\n",
    "    \n",
    "print(f\"Vocabulary mapping for given sample corpus : \\n {sample_bow.vocabulary_}\")\n",
    "print(\"\\nBag of word Representation of sentence 'the cat cat sat in the hat'\")\n",
    "print(get_bow_representation([\"the cat cat sat in the hat\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T07:08:34.889920Z",
     "start_time": "2023-04-04T07:08:34.774426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  000114  000419  000ft  000lbs  0011  0016  00a  00am  ...  \\\n",
      "0   0    0       0       0      0       0     0     0    0     0  ...   \n",
      "1   0    0       0       0      0       0     0     0    0     0  ...   \n",
      "2   0    0       0       0      0       0     0     0    0     0  ...   \n",
      "3   0    0       0       0      0       0     0     0    0     0  ...   \n",
      "4   0    0       0       0      0       0     0     0    0     0  ...   \n",
      "\n",
      "   zrh_airport  zsdgzydnd  zsuztnaijq  ztrdwv0n4l  zuke  zurich  zv2pt6trk9  \\\n",
      "0            0          0           0           0     0       0           0   \n",
      "1            0          0           0           0     0       0           0   \n",
      "2            0          0           0           0     0       0           0   \n",
      "3            0          0           0           0     0       0           0   \n",
      "4            0          0           0           0     0       0           0   \n",
      "\n",
      "   zv6cfpohl5  zvfmxnuelj  zzps5ywve2  \n",
      "0           0           0           0  \n",
      "1           0           0           0  \n",
      "2           0           0           0  \n",
      "3           0           0           0  \n",
      "4           0           0           0  \n",
      "\n",
      "[5 rows x 12226 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebdl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 현재 전처리된 데이터셋을 가지고 data의 text 부분을 \n",
    "# bag-of-words 형태로 변환하고 해당 데이터프레임에 저장하고 일부를 출력해보아라.\n",
    "\n",
    "# CountVectorizer 객체 생성\n",
    "bow = CountVectorizer()\n",
    "\n",
    "# 데이터셋에서 text 부분 추출\n",
    "texts = data_vocab\n",
    "\n",
    "# bag-of-words 형태로 변환\n",
    "bow_repr = bow.fit_transform(texts)\n",
    "\n",
    "# 변환된 결과를 데이터프레임에 저장\n",
    "bow_df = pd.DataFrame(bow_repr.toarray(), columns=bow.get_feature_names())\n",
    "\n",
    "# 일부 출력\n",
    "print(bow_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words의 장점과 단점\n",
    "- 구현이 쉽고 빠른 장점이 있는 것 같고 어떤 단어가 들어가도 정량적인 특성을 가진 벡터로 나타낼 수 있는 장점이 있습니다.\n",
    "- 단점으로는 단순히 단어의 빈도만을 계산하기 때문에 문맥, 순서등을 고려하지 않는 점과 벡터의 희소성이 높아질 우려가 있다는 점이있습니다.\n",
    "\n",
    "### One-hot Encoding과의 유사점과 차이점은 무엇인가?\n",
    "- 둘다 텍스트 데이터를 수치형 벡터로 변환하는데 사용한다는 점에서 비슷합니다.\n",
    "- 차이점으로는 방식의 차이가 가장 큰데 원핫인코딩은 vocab만큼의 벡터 크기를 가지게 되고 해당 단어만 1이고 나머지는 0인 형태를 가지는 반면 BOW 모델은 단어의 출현 빈도를 벡터로 표현하고 마찬가지로 단어만큼의 차원을 가집니다.\n",
    "\n",
    "### Bag-of-Words 방식의 장점과 단점은 무엇인가?\n",
    "- 단어 간 유사도를 계산하여 더욱 정교한 표현이 가능하다\n",
    "- 단어의 순서 정보가 손실되어 텍스트 분류에서 성능이 좋지 않을 수 있다 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
