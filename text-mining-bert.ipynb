{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izA3-6kffbdT",
    "tags": []
   },
   "source": [
    "# BERT를 처음 사용하기 위한 시각적 가이드 및 분류 문제 해결\n",
    "\n",
    "- 이번 과제에서는 BERT 모델을 설치해보고 이를 활용하여 모델을 불러오고, 분류 문제를 풀기 위해 모델을 학습해본다.\n",
    "- 문장 2000개를 DistilBERT를 통해 전처리 (BertEmbedding) 결과를 받아 문장을 긍정 또는 부정 (각각 1 또는 0)으로 분류하는 간단한 Logistic Regression을 구현한다.\n",
    "- 이번 과제는 아래의 튜토리얼에서 발췌되었고, 과제의 보다 자세한 내용을 위해서 아래 튜토리얼을 참고하면 됩니다.\n",
    "\n",
    "- http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ [영어]\n",
    "- https://chloamme.github.io/2021/12/22/a-visual-guide-to-using-bert-for-the-first-time-korean.html [한국어]\n",
    "\n",
    "- 이번 과제에서 풀어야 할 내용\n",
    "- 0. 튜토리얼 실행 [10점]\n",
    "- 1. 튜토리얼에 이용한 전처리 문장 수를 2000개에서 전체 6915개를 모두 이용하여 이전 Logistic Regression 모델의 결과와 비교한다. [10점]\n",
    "- 2. DistilBERT를 이용해 전처리된 내용을 Logistic Regression 외에 LSTM, GRU, BertForSequenceClassification (택1) 등을 이용하여 Logistic Regression의 성능과 비교한다. [20점]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 문장 감정 분류\n",
    " \n",
    "- 우리의 목표는 (우리의 데이터셋과 같은) 문장을 받아서 (긍정적 감정을 가진 문장을 의미하는) 1 또는 (부정적 감정을 가진 문장을 의미하는) 0을 생성하는 것입니다. 다음과 같이 생겼다고 생각할 수 있습니다:\n",
    "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
    "\n",
    "- 모델은 사실 내부적으로 두개의 모델로 구성되어 있습니다.\n",
    " \n",
    "- DistilBERT는 문장을 처리하고, 문장에서 추출한 몇가지 정보를 다음 모델에 전달합니다. DistilBERT는 HuggingFace 팀에서 개발하고 오픈소스로 제공하는 BERT의 작은 버전입니다. 성능은 BERT와 대략 비슷하지만 가볍고 빠른 버전입니다. 다음 모델인 scikit learn의 기본 Logistic Regression 모델은 DistilBERT 처리 결과를 받아 문장을 긍정 또는 부정 (각각 1 또는 0)으로 분류합니다.\n",
    " \n",
    "- 두 모델 간 전달하는 데이터는 768 크기의 벡터입니다. 벡터를 분류하려는 문장에 대한 임베딩으로 생각할 수 있습니다.\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "이 예제에서 사용할 데이터셋은 SST2이고, 이 것은 영화 리뷰 문장들과, 각 문장 별 긍정(값이 1) 또는 부정(값이 0)으로 레이블링 되어 있습니다:\n",
    "\n",
    "\n",
    "<table class=\"features-table\">\n",
    "  <tr>\n",
    "    <th class=\"mdc-text-light-green-600\">\n",
    "    sentence\n",
    "    </th>\n",
    "    <th class=\"mdc-text-purple-600\">\n",
    "    label\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      apparently reassembled from the cutting room floor of any given daytime soap\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      they presume their audience won't sit still for a sociology lesson\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ-42fh0hjsF"
   },
   "source": [
    "## Importing the dataset\n",
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:30.551663Z",
     "start_time": "2023-05-19T08:08:29.943706Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cyoj29J24hPX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('sst-train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:30.566615Z",
     "start_time": "2023-05-19T08:08:30.552661Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "0     a stirring , funny and finally transporting re...  1\n",
       "1     apparently reassembled from the cutting room f...  0\n",
       "2     they presume their audience wo n't sit still f...  0\n",
       "3     this is a visually stunning rumination on love...  1\n",
       "4     jonathan parker 's bartleby should have been t...  1\n",
       "...                                                 ... ..\n",
       "6915  painful , horrifying and oppressively tragic ,...  1\n",
       "6916  take care is nicely performed by a quintet of ...  0\n",
       "6917  the script covers huge , heavy topics in a bla...  0\n",
       "6918  a seriously bad film with seriously warped log...  0\n",
       "6919  a deliciously nonsensical comedy about a city ...  1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMVE3waNhuNj"
   },
   "source": [
    "본 과제에서는 2000개의 문장만 이용하기로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:30.581563Z",
     "start_time": "2023-05-19T08:08:30.567611Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gTM3hOHW4hUY",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1 = df[:2000]\n",
    "batch_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRc2L89hh1Tf"
   },
   "source": [
    "- Dataframe을 이용해 얼마나 많은 문장이 \"긍정적\"(값 1)으로 표시되고 얼마나 많은 문장이 \"부정적\"(값 0)으로 표시되는지 value_counts()라는 함수를 이용해 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:30.596513Z",
     "start_time": "2023-05-19T08:08:30.582560Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jGvcfcCP5xpZ",
    "outputId": "4c4a8afc-1035-4b21-ba9a-c4bb6cfc6347",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1041\n",
       "0     959\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the transformers library\n",
    "- 딥러닝 NLP 모델을 로드할 수 있도록 pytorch와 huggingface transformer 라이브러리를 설치하는 것으로 시작하겠습니다.\n",
    "- pytorch의 경우 OS에 따라 https://pytorch.org/get-started/locally/ 링크를 참고하여 설치할 수 있도록 한다.\n",
    "- 이미 라이브러리를 설치 했다면 이 과정은 건너 뛰어도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:30.611463Z",
     "start_time": "2023-05-19T08:08:30.597510Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "To9ENLU90WGl",
    "outputId": "4b46c997-c16c-4141-eaf2-e7aa7da6d3a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CUDA 설치 for Windows\n",
    "# 학습을 위한 그래픽 카드를 사용하기 위한 CUDA 설치는 아래 링크를 참고\n",
    "# https://afsdzvcx123.tistory.com/entry/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-Windows%EC%9C%88%EB%8F%84%EC%9A%B0-CUDA-cuDNN-%EC%84%A4%EC%B9%98%EB%B0%A9%EB%B2%95\n",
    "\n",
    "# pytorch - https://pytorch.org/get-started/locally/\n",
    "#pytorch 설치 for Windows - 2.4GB 정도의 되는 라이브러리를 설치함. 인터넷 상태에 따라 10분 이상 걸림\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "\n",
    "# transformers 설치\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "## Loading the Pre-trained BERT model\n",
    "Let's now load a pre-trained BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:34.638307Z",
     "start_time": "2023-05-19T08:08:30.612461Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "q1InADgf5xm2",
    "outputId": "dbc52856-4d52-42f8-8a74-a89944280a02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## distilBERT 대신 BERT를 사용하고 싶으신가요? 그럼 아래의 주석처리를 해제하세요\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "# 현재 'model' 변수는 사전 훈련된 distilBERT 모델을 보유하고 있습니다. 이 모델은 더 작지만 훨씬 빠르고 훨씬 적은 메모리를 필요로 하는 BERT 버전입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZDBMn3wiSX6"
   },
   "source": [
    "## Model #1: Preparing the Dataset\n",
    "- 문장을 BERT에 전달하기 전에 필요한 형식으로 문장을 넣기 위한 최소한의 처리가 필요합니다. 이를 Tokenization이라고 부릅니다.\n",
    "\n",
    "### Tokenization\n",
    "첫 번째 단계는 문장을 토큰화하는 것입니다. BERT가 편한 형식으로 데이터셋에 있는 문장을 word와 sub word로 분해합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:35.117129Z",
     "start_time": "2023-05-19T08:08:34.639304Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "1995    [101, 2205, 20857, 1998, 11865, 16643, 2135, 5...\n",
       "1996    [101, 2009, 2515, 1050, 1005, 1056, 2147, 2004...\n",
       "1997    [101, 2023, 2028, 8704, 2005, 1996, 11848, 199...\n",
       "1998    [101, 1999, 1996, 2171, 1997, 2019, 9382, 1898...\n",
       "1999    [101, 1996, 3185, 2003, 25757, 2011, 1037, 244...\n",
       "Name: 0, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHwjUwYgi-uL"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n",
    "\n",
    "### Padding\n",
    "- 토큰화 후 `tokenized`는 문장의 목록입니다. 각 문장은 토큰 목록으로 표시됩니다. 우리는 BERT가 예제를 한 번에(하나의 배치로) 처리하기를 원합니다. 그렇게 하면 더 빠를 뿐입니다. 이러한 이유로 모든 목록을 동일한 크기로 채워야 입력을 (길이가 다른) 목록 목록이 아닌 하나의 2-d 배열로 나타낼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:35.132079Z",
     "start_time": "2023-05-19T08:08:35.118127Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "URn-DWJt5xhP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
       "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
       "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  2023,  2028, ...,     0,     0,     0],\n",
       "       [  101,  1999,  1996, ...,     0,     0,     0],\n",
       "       [  101,  1996,  3185, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mdjg306wjjmL"
   },
   "source": [
    "- tokenized와 padding을 거친 데이터는 이제 `padded` 변수에 있으며 아래에서 해당 크기를 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:35.147029Z",
     "start_time": "2023-05-19T08:08:35.133076Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jdi7uXo95xeq",
    "outputId": "be786022-e84f-4e28-8531-0143af2347bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDZBsYSDjzDV"
   },
   "source": [
    "### Masking\n",
    "- BERT에 `padded`를 직접 보내면 약간 혼란스러울 것입니다. 입력을 처리할 때 추가한 패딩을 무시(마스크)하도록 다른 변수를 만들어야 합니다. 이것이 바로 attention_mask입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:08:35.161978Z",
     "start_time": "2023-05-19T08:08:35.148026Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K_iGRNa_Ozc",
    "outputId": "d03b0a9b-1f6e-4e32-831e-b04f5389e57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jK-CQB9-kN99"
   },
   "source": [
    "## Model #1: And Now, Deep Learning!\n",
    "- 이제 모델과 전처리 된 입력 (tokenized, padded, attention mask) 이 준비되었으므로 모델을 실행해 보겠습니다.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />\n",
    "\n",
    "- `model()` 함수는 BERT를 통해 문장을 실행합니다. 처리 결과는 `last_hidden_states`로 반환됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.698441Z",
     "start_time": "2023-05-19T08:08:35.162976Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoCep_WVuB3v"
   },
   "source": [
    "- 출력에서 필요한 부분만 슬라이스해 보겠습니다. 그것은 각 문장의 첫 번째 토큰에 해당하는 출력입니다. BERT가 문장 분류를 수행하는 방식은 모든 문장의 시작 부분에 `[CLS]`(분류용)라는 토큰을 추가하는 것입니다. 해당 토큰에 해당하는 출력은 전체 문장에 대한 임베딩으로 생각할 수 있습니다.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
    "\n",
    "- 이후에 사용할 로지틱스 회귀 모델의 기능으로 `last_hidden_states`의 정보가 사용되므로 `features` 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.728341Z",
     "start_time": "2023-05-19T08:09:45.700435Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "C9t60At16PVs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21593435, -0.14028911,  0.00831076, ..., -0.13694832,\n",
       "         0.5867005 ,  0.20112693],\n",
       "       [-0.17262718, -0.14476153,  0.00223438, ..., -0.1744257 ,\n",
       "         0.21386446,  0.37197465],\n",
       "       [-0.05063373,  0.07203954, -0.02959727, ..., -0.0714895 ,\n",
       "         0.7185238 ,  0.2622547 ],\n",
       "       ...,\n",
       "       [-0.27829778, -0.24803615,  0.13585788, ..., -0.19039172,\n",
       "         0.13099575,  0.3497837 ],\n",
       "       [-0.03667723,  0.10638573, -0.01111037, ..., -0.1120664 ,\n",
       "         0.41619483,  0.5033802 ],\n",
       "       [ 0.12402646,  0.01425165,  0.01038418, ..., -0.11606564,\n",
       "         0.5345917 ,  0.27495325]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VZVU66Gurr-"
   },
   "source": [
    "- 어떤 문장이 긍정적이고 부정적인지를 나타내는 레이블은 이제 `labels` 변수로 이동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.743291Z",
     "start_time": "2023-05-19T08:09:45.730334Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JD3fX2yh6PTx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1995    0\n",
       "1996    0\n",
       "1997    0\n",
       "1998    0\n",
       "1999    0\n",
       "Name: 1, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch_1[1]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "## Model #2: Train/Test Split\n",
    "- 이제 데이터 세트를 훈련 세트와 테스트 세트로 분할하겠습니다(SST2 훈련 세트에서 2,000개의 문장)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.758241Z",
     "start_time": "2023-05-19T08:09:45.745284Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9bhSJpcv1Bl"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.773190Z",
     "start_time": "2023-05-19T08:09:45.761231Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cyEwr7yYD3Ci"
   },
   "outputs": [],
   "source": [
    "### [Bonus] Grid Search for Parameters\n",
    "# We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength.\n",
    "\n",
    "# parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "# grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "# grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# print('best parameters: ', grid_search.best_params_)\n",
    "# print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCT9u8vAwnID"
   },
   "source": [
    "- 이제 LogisticRegression 모델을 훈련합니다. 그리드 검색을 수행하도록 선택한 경우 C 값을 모델 선언에 연결할 수 있습니다(예: `LogisticRegression(C=5.2)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.937638Z",
     "start_time": "2023-05-19T08:09:45.774191Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gG-EVWx4CzBc",
    "outputId": "9252ceff-a7d0-4359-fef9-2f72be89c7d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3rUMKuVgwzkY"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png\" />\n",
    "\n",
    "## Evaluating Model #2\n",
    "- 그렇다면 우리 모델이 문장을 분류하는 데 얼마나 잘 작동할까요? 한 가지 방법은 테스트 데이터 세트에 대해 정확도를 확인하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.952588Z",
     "start_time": "2023-05-19T08:09:45.938635Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCoyxRJ7ECTA",
    "outputId": "cfd86dea-5d16-476c-ab9b-47cbee3a014f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 문장 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전체 문장을 다 이용하는 batch_2 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:45.997439Z",
     "start_time": "2023-05-19T08:09:45.957572Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('sst-train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:46.012387Z",
     "start_time": "2023-05-19T08:09:46.001426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2 = df\n",
    "batch_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:46.027337Z",
     "start_time": "2023-05-19T08:09:46.016374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전체 문장에 대해서 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:49.017326Z",
     "start_time": "2023-05-19T08:09:46.029331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "6915    [101, 9145, 1010, 7570, 18752, 14116, 1998, 28...\n",
       "6916    [101, 2202, 2729, 2003, 19957, 2864, 2011, 103...\n",
       "6917    [101, 1996, 5896, 4472, 4121, 1010, 3082, 7832...\n",
       "6918    [101, 1037, 5667, 2919, 2143, 2007, 5667, 2561...\n",
       "6919    [101, 1037, 12090, 2135, 2512, 5054, 19570, 23...\n",
       "Name: 0, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_2 = batch_2[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:49.048224Z",
     "start_time": "2023-05-19T08:09:49.022311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "6915    [101, 9145, 1010, 7570, 18752, 14116, 1998, 28...\n",
       "6916    [101, 2202, 2729, 2003, 19957, 2864, 2011, 103...\n",
       "6917    [101, 1996, 5896, 4472, 4121, 1010, 3082, 7832...\n",
       "6918    [101, 1037, 5667, 2919, 2143, 2007, 5667, 2561...\n",
       "6919    [101, 1037, 12090, 2135, 2512, 5054, 19570, 23...\n",
       "Name: 0, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전체 문장을 다 사용하는 token에 padded_2 추가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:49.108025Z",
     "start_time": "2023-05-19T08:09:49.049221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
       "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
       "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1996,  5896, ...,     0,     0,     0],\n",
       "       [  101,  1037,  5667, ...,     0,     0,     0],\n",
       "       [  101,  1037, 12090, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized_2.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_2 = np.array([i + [0]*(max_len-len(i)) for i in tokenized_2.values])\n",
    "padded_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:49.122973Z",
     "start_time": "2023-05-19T08:09:49.110020Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded_2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전체 문장에 대한 attention_mask_2 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:09:49.137923Z",
     "start_time": "2023-05-19T08:09:49.123970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_2 = np.where(padded_2 != 0, 1, 0)\n",
    "attention_mask_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model() 함수는 DistillBERT를 통해 문장을 실행합니다. \n",
    "- 처리 결과는 last_hidden_states_2로 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.010490Z",
     "start_time": "2023-05-19T08:09:49.138922Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids_2 = torch.tensor(padded_2)  \n",
    "attention_mask_2 = torch.tensor(attention_mask_2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states_2 = model(input_ids_2, attention_mask=attention_mask_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이후에 사용할 로지틱스 회귀 모델의 기능으로 last_hidden_states_2의 정보가 사용되므로 features_2 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.025439Z",
     "start_time": "2023-05-19T08:14:41.013480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21593435, -0.14028911,  0.00831076, ..., -0.13694832,\n",
       "         0.5867005 ,  0.20112693],\n",
       "       [-0.17262718, -0.14476153,  0.00223438, ..., -0.1744257 ,\n",
       "         0.21386446,  0.37197465],\n",
       "       [-0.05063373,  0.07203954, -0.02959727, ..., -0.0714895 ,\n",
       "         0.7185238 ,  0.2622547 ],\n",
       "       ...,\n",
       "       [-0.06550944, -0.0518475 , -0.14094469, ..., -0.06450683,\n",
       "         0.60223037,  0.21347877],\n",
       "       [-0.08523139, -0.04869805, -0.08137526, ..., -0.13589348,\n",
       "         0.39505622,  0.22889704],\n",
       "       [-0.29436833, -0.09234679, -0.00831667, ..., -0.05159124,\n",
       "         0.43497816,  0.28891575]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_2 = last_hidden_states_2[0][:,0,:].numpy()\n",
    "features_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.085238Z",
     "start_time": "2023-05-19T08:14:41.028429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.2159, -0.1403,  0.0083,  ..., -0.1369,  0.5867,  0.2011],\n",
       "         [-0.2471,  0.2468,  0.1008,  ..., -0.1631,  0.9349, -0.0715],\n",
       "         [ 0.0558,  0.3573,  0.4140,  ..., -0.2430,  0.1770, -0.5080],\n",
       "         ...,\n",
       "         [ 0.1864,  0.0193,  0.1864,  ..., -0.2175,  0.1604, -0.4050],\n",
       "         [-0.1004,  0.0651,  0.1240,  ..., -0.1649,  0.3568,  0.1218],\n",
       "         [-0.0114,  0.3297,  0.2317,  ..., -0.2362,  0.4217,  0.0895]],\n",
       "\n",
       "        [[-0.1726, -0.1448,  0.0022,  ..., -0.1744,  0.2139,  0.3720],\n",
       "         [ 0.0022,  0.1684,  0.1269,  ..., -0.1888, -0.0195, -0.0283],\n",
       "         [ 0.0257, -0.2458,  0.0717,  ..., -0.4339,  0.1622,  0.0133],\n",
       "         ...,\n",
       "         [ 0.0466,  0.0850,  0.1801,  ..., -0.0279,  0.1878,  0.4022],\n",
       "         [-0.2325,  0.0746,  0.1298,  ..., -0.1292,  0.0904,  0.3647],\n",
       "         [-0.0655, -0.2214,  0.1827,  ..., -0.1624,  0.1421,  0.0963]],\n",
       "\n",
       "        [[-0.0506,  0.0720, -0.0296,  ..., -0.0715,  0.7185,  0.2623],\n",
       "         [ 0.0536,  0.3136, -0.0598,  ...,  0.2676,  0.8668, -0.3380],\n",
       "         [ 0.3792,  0.2792,  0.0237,  ...,  0.0343,  0.4272,  0.1680],\n",
       "         ...,\n",
       "         [ 0.2295,  0.0179,  0.1896,  ..., -0.0725,  0.1776,  0.0065],\n",
       "         [ 0.4776,  0.0545,  0.0732,  ..., -0.1118,  0.2219, -0.0038],\n",
       "         [ 0.2757,  0.1092,  0.0957,  ..., -0.1413,  0.1903,  0.0480]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0655, -0.0518, -0.1409,  ..., -0.0645,  0.6022,  0.2135],\n",
       "         [-0.1388, -0.2643, -0.3730,  ...,  0.1416,  0.7465, -0.4062],\n",
       "         [ 0.4451, -0.0495, -0.2538,  ..., -0.1317,  0.0744, -0.2182],\n",
       "         ...,\n",
       "         [ 0.0121,  0.0664,  0.1355,  ..., -0.1351,  0.0596,  0.4312],\n",
       "         [ 0.2858,  0.1074, -0.0675,  ..., -0.2254,  0.4139,  0.1925],\n",
       "         [-0.0242,  0.2324,  0.1520,  ..., -0.2130,  0.2117,  0.3134]],\n",
       "\n",
       "        [[-0.0852, -0.0487, -0.0814,  ..., -0.1359,  0.3951,  0.2289],\n",
       "         [-0.2409, -0.1853, -0.2926,  ..., -0.1778,  0.8022,  0.1119],\n",
       "         [ 0.1641, -0.0146, -0.0866,  ..., -0.4101,  0.3737, -0.2818],\n",
       "         ...,\n",
       "         [ 0.0313, -0.0616, -0.0388,  ..., -0.1146,  0.2336, -0.1422],\n",
       "         [ 0.0923, -0.0839,  0.0963,  ..., -0.0719,  0.2545, -0.0018],\n",
       "         [ 0.0867,  0.1704,  0.3577,  ..., -0.0636,  0.0836, -0.1998]],\n",
       "\n",
       "        [[-0.2944, -0.0923, -0.0083,  ..., -0.0516,  0.4350,  0.2889],\n",
       "         [-0.3361,  0.1819, -0.1328,  ..., -0.1553,  0.8175,  0.2301],\n",
       "         [-0.0039, -0.1527,  0.2471,  ..., -0.3899,  0.3394,  0.0263],\n",
       "         ...,\n",
       "         [-0.1473, -0.0756,  0.1934,  ..., -0.0059, -0.0552, -0.2198],\n",
       "         [-0.0198, -0.2047,  0.1223,  ..., -0.2881,  0.0755, -0.1100],\n",
       "         [ 0.1322, -0.4463,  0.1558,  ..., -0.1565,  0.2923, -0.0541]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.100189Z",
     "start_time": "2023-05-19T08:14:41.087232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "6915    1\n",
       "6916    0\n",
       "6917    0\n",
       "6918    0\n",
       "6919    1\n",
       "Name: 1, Length: 6920, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_2 = batch_2[1]\n",
    "labels_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 데이터 세트를 훈련 세트와 테스트 세트로 분할하겠습니다(SST2 훈련 세트에서 6920개의 문장)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.115139Z",
     "start_time": "2023-05-19T08:14:41.102181Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features_2, test_features_2, train_labels_2, test_labels_2 = train_test_split(features_2, labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.563632Z",
     "start_time": "2023-05-19T08:14:41.116136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_2 = LogisticRegression()\n",
    "lr_clf_2.fit(train_features_2, train_labels_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**테스트 데이터 정확도 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.578830Z",
     "start_time": "2023-05-19T08:14:41.565008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572254335260115"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_2.score(test_features_2, test_labels_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 결과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2000개의 데이터를 사용했을때 **81.8**의 score나왔는데, 전체 데이터를 다 사용하니 **85.02890173410405**의 score로 성능이 향상되었습니다.(모델 돌릴때마다 값이 조금 바뀌어서 처음 나온 결과로 작성하였습니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.638630Z",
     "start_time": "2023-05-19T08:14:41.581820Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.668529Z",
     "start_time": "2023-05-19T08:14:41.640623Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('sst-train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.773177Z",
     "start_time": "2023-05-19T08:14:41.670523Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.803079Z",
     "start_time": "2023-05-19T08:14:41.775171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "0     a stirring , funny and finally transporting re...  1\n",
       "1     apparently reassembled from the cutting room f...  0\n",
       "2     they presume their audience wo n't sit still f...  0\n",
       "3     this is a visually stunning rumination on love...  1\n",
       "4     jonathan parker 's bartleby should have been t...  1\n",
       "...                                                 ... ..\n",
       "6915  painful , horrifying and oppressively tragic ,...  1\n",
       "6916  take care is nicely performed by a quintet of ...  0\n",
       "6917  the script covers huge , heavy topics in a bla...  0\n",
       "6918  a seriously bad film with seriously warped log...  0\n",
       "6919  a deliciously nonsensical comedy about a city ...  1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:41.818029Z",
     "start_time": "2023-05-19T08:14:41.807064Z"
    }
   },
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:3900][0].values\n",
    "train_labels = df.iloc[:3900][1].values\n",
    "\n",
    "valid_texts = df.iloc[3900:5200][0].values\n",
    "valid_labels = df.iloc[3900:5200][1].values\n",
    "\n",
    "test_texts = df.iloc[5200:][0].values\n",
    "test_labels = df.iloc[5200:][1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:42.565516Z",
     "start_time": "2023-05-19T08:14:41.821017Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:42.804712Z",
     "start_time": "2023-05-19T08:14:42.566514Z"
    }
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:42.819663Z",
     "start_time": "2023-05-19T08:14:42.806706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=66, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:42.849562Z",
     "start_time": "2023-05-19T08:14:42.829629Z"
    }
   },
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:42.864516Z",
     "start_time": "2023-05-19T08:14:42.852553Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:42.879462Z",
     "start_time": "2023-05-19T08:14:42.866505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:43.940896Z",
     "start_time": "2023-05-19T08:14:42.881456Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:14:43.955846Z",
     "start_time": "2023-05-19T08:14:43.943886Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "            ### Prepare data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += labels.size(0)\n",
    "\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:15:59.056706Z",
     "start_time": "2023-05-19T08:14:43.957839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0003 | Batch 0000/0244 | Loss: 0.7045\n",
      "training accuracy: 95.59%\n",
      "valid accuracy: 86.31%\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 0002/0003 | Batch 0000/0244 | Loss: 0.2252\n",
      "training accuracy: 98.38%\n",
      "valid accuracy: 84.62%\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 0003/0003 | Batch 0000/0244 | Loss: 0.2105\n",
      "training accuracy: 99.49%\n",
      "valid accuracy: 86.08%\n",
      "Time elapsed: 1.22 min\n",
      "Total Training Time: 1.22 min\n",
      "Test accuracy: 87.67%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        ### Logging\n",
    "        if not batch_idx % 250:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "            \n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BertForSequenceClassification로 모델링한 결과 3 에포크만에 87.67%의 Test accuracy를 보이며 Logistic Regression 방식보다 좋은 성능을 보였습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Lg4LOpoxSOR"
   },
   "source": [
    "## Proper SST2 scores\n",
    "참고로 이 데이터셋의 [최고 정확도 점수](http://nlpprogress.com/english/sentiment_analysis.html)는 현재 **96.8**입니다. DistilBERT는 이 작업에서 점수를 향상시키도록 훈련될 수 있습니다. 이 프로세스는 이 문장 분류 작업(다운스트림 작업이라고 부를 수 있음)에서 더 나은 성능을 달성할 수 있도록 BERT의 가중치를 업데이트하는 **Fine Tuning**이라는 프로세스입니다. 미세 조정된 DistilBERT는 **90.7**의 정확도 점수를 달성하는 것으로 나타났습니다. 전체 크기 BERT 모델은 **94.9**를 달성합니다.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "A Visual Notebook to Using BERT for the First Time.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.994px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
